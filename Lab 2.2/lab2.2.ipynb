{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self):\n",
    "        self.grid_size = (3, 3)\n",
    "        self.start_state = (0, 0)\n",
    "        self.goal_state = (2, 2)\n",
    "        self.num_actions = 4  # Up, Down, Left, Right\n",
    "    \n",
    "    def step(self, state, action):\n",
    "        \"\"\"Return the next state and reward given the current state and action.\"\"\"\n",
    "        x, y = state\n",
    "\n",
    "        if action == 0:  # Up\n",
    "            next_state = (max(0, x - 1), y)\n",
    "        elif action == 1:  # Down\n",
    "            next_state = (min(self.grid_size[0] - 1, x + 1), y)\n",
    "        elif action == 2:  # Left\n",
    "            next_state = (x, max(0, y - 1))\n",
    "        elif action == 3:  # Right\n",
    "            next_state = (x, min(self.grid_size[1] - 1, y))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action\")\n",
    "\n",
    "        # Check if the agent reaches the goal state\n",
    "        reward = 10 if next_state == self.goal_state else -1\n",
    "        return next_state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m  \u001b[38;5;66;03m# Learning rate\u001b[39;00m\n\u001b[0;32m     31\u001b[0m gamma \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.9\u001b[39m  \u001b[38;5;66;03m# Discount factor\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[43mtd_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid_world\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Print the learned value function\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValue function:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m, in \u001b[0;36mtd_learning\u001b[1;34m(grid_world, num_episodes, alpha, gamma)\u001b[0m\n\u001b[0;32m      7\u001b[0m state \u001b[38;5;241m=\u001b[39m grid_world\u001b[38;5;241m.\u001b[39mstart_state\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m state \u001b[38;5;241m!=\u001b[39m grid_world\u001b[38;5;241m.\u001b[39mgoal_state:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Choose a random action (can later be improved with exploration-exploitation)\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid_world\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# Get the next state and reward from the environment\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     next_state, reward \u001b[38;5;241m=\u001b[39m grid_world\u001b[38;5;241m.\u001b[39mstep(state, action)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def td_learning(grid_world, num_episodes, alpha, gamma):\n",
    "    \"\"\"Perform Temporal Difference learning to estimate the value function.\"\"\"\n",
    "    # Initialize the value function to 0 for all states\n",
    "    values = np.zeros(grid_world.grid_size)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = grid_world.start_state\n",
    "\n",
    "        while state != grid_world.goal_state:\n",
    "            # Choose a random action (can later be improved with exploration-exploitation)\n",
    "            action = np.random.choice(grid_world.num_actions)\n",
    "\n",
    "            # Get the next state and reward from the environment\n",
    "            next_state, reward = grid_world.step(state, action)\n",
    "\n",
    "            # TD(0) update rule for the value function\n",
    "            values[state] += alpha * (reward + gamma * values[next_state] - values[state])\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "    return values\n",
    "\n",
    "\n",
    "# Create a grid world environment\n",
    "grid_world = GridWorld()\n",
    "\n",
    "# Perform Temporal Difference learning\n",
    "num_episodes = 1000\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "values = td_learning(grid_world, num_episodes, alpha, gamma)\n",
    "\n",
    "# Print the learned value function\n",
    "print(\"Value function:\")\n",
    "print(values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
