{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "albKKc7Mv7ig",
        "outputId": "7b282709-a7a0-4a94-d94e-c4a7f24fce20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " State = (0, 0), Action = 3\n",
            " State = (0, 1), Action = 1\n",
            " State = (1, 1), Action = 1\n",
            " State = (2, 1), Action = 3\n",
            "Total reward obtained by learned policy: 1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class GridWorld:\n",
        "    def __init__(self):\n",
        "        self.grid_size = (3, 3)\n",
        "        self.num_actions = 4  # Up, Down, Left, Right\n",
        "        self.start_state = (0, 0)\n",
        "        self.goal_state = (2, 2)\n",
        "\n",
        "    def step(self, state, action):\n",
        "        row, col = state\n",
        "        if action == 0:  # Up\n",
        "            row = max(0, row - 1)\n",
        "        elif action == 1:  # Down\n",
        "            row = min(self.grid_size[0] - 1, row + 1)\n",
        "        elif action == 2:  # Left\n",
        "            col = max(0, col - 1)\n",
        "        elif action == 3:  # Right\n",
        "            col = min(self.grid_size[1] - 1, col + 1)\n",
        "        next_state = (row, col)\n",
        "        reward = 1 if next_state == self.goal_state else 0\n",
        "        return next_state, reward\n",
        "\n",
        "class ActorCritic:\n",
        "    def __init__(self, num_actions, alpha_actor, alpha_critic, gamma):\n",
        "        self.num_actions = num_actions\n",
        "        self.alpha_actor = alpha_actor\n",
        "        self.alpha_critic = alpha_critic\n",
        "        self.gamma = gamma\n",
        "        self.actor_params = np.zeros((3, 3, num_actions))  # Tabular actor parameters\n",
        "        self.critic_values = np.zeros((3, 3))  # Tabular critic values\n",
        "\n",
        "    def select_action(self, state):\n",
        "        # Use softmax to select an action probabilistically based on actor parameters\n",
        "        row, col = state\n",
        "        probabilities = self.softmax(self.actor_params[row, col])\n",
        "        action = np.random.choice(self.num_actions, p=probabilities)\n",
        "        return action\n",
        "\n",
        "    def update(self, state, action, reward, next_state):\n",
        "        row, col = state\n",
        "        next_row, next_col = next_state\n",
        "\n",
        "        # Compute TD error (advantage)\n",
        "        td_target = reward + self.gamma * self.critic_values[next_row, next_col]\n",
        "        td_error = td_target - self.critic_values[row, col]\n",
        "\n",
        "        # Update critic values\n",
        "        self.critic_values[row, col] += self.alpha_critic * td_error\n",
        "\n",
        "        # Update actor parameters\n",
        "        self.actor_params[row, col, action] += self.alpha_actor * td_error * (1 - self.softmax(self.actor_params[row, col])[action])\n",
        "\n",
        "    def softmax(self, x):\n",
        "        e_x = np.exp(x - np.max(x))\n",
        "        return e_x / e_x.sum()\n",
        "\n",
        "# Create a grid world environment\n",
        "grid_world = GridWorld()\n",
        "\n",
        "# Create an Actor-Critic agent\n",
        "num_actions = 4  # Up, Down, Left, Right\n",
        "alpha_actor = 0.1\n",
        "alpha_critic = 0.1\n",
        "gamma = 0.9\n",
        "actor_critic_agent = ActorCritic(num_actions, alpha_actor, alpha_critic, gamma)\n",
        "\n",
        "# Train the Actor-Critic agent\n",
        "num_episodes = 1000\n",
        "for _ in range(num_episodes):\n",
        "    state = grid_world.start_state\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = actor_critic_agent.select_action(state)\n",
        "        next_state, reward = grid_world.step(state, action)\n",
        "        actor_critic_agent.update(state, action, reward, next_state)\n",
        "\n",
        "        if next_state == grid_world.goal_state:\n",
        "            done = True\n",
        "        state = next_state\n",
        "\n",
        "# Evaluate the learned policy\n",
        "total_reward = 0\n",
        "state = grid_world.start_state\n",
        "while state != grid_world.goal_state:\n",
        "    action = actor_critic_agent.select_action(state)\n",
        "    print(f\" State = {state}, Action = {action}\")\n",
        "    next_state, reward = grid_world.step(state, action)\n",
        "    total_reward += reward\n",
        "    state = next_state\n",
        "\n",
        "print(\"Total reward obtained by learned policy:\", total_reward)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLHwXoNtv7Ve"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
